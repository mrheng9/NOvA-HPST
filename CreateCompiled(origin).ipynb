{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589abd87-d67e-4290-bc44-210064d605c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/exp/dune/data/users/ayankele/transformercvn_env/lib/python3.10/site-packages/MinkowskiEngine/__init__.py:36: UserWarning: The environment variable `OMP_NUM_THREADS` not set. MinkowskiEngine will automatically set `OMP_NUM_THREADS=16`. If you want to set `OMP_NUM_THREADS` manually, please export it on the command line before running a python script. e.g. `export OMP_NUM_THREADS=12; python your_program.py`. It is recommended to set it below 24.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from glob import glob\n",
    "import re\n",
    "\n",
    "from transformercvn.options import Options\n",
    "from transformercvn.network.trainers.neutrino_full_dense_trainer import NeutrinoFullDenseTrainer, sparse_to_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b5a4a6-4273-43f5-b259-c716f9f63ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA = False\n",
    "CUDA_DEVICE = 0\n",
    "\n",
    "CHECKPOINT_PATH = \"./tutorial_dense/version_0/checkpoints/epoch=0-step=28500.ckpt\" # Model to export\n",
    "NETWORK = NeutrinoFullDenseTrainer\n",
    "TESTING_SOURCE = \"training\"\n",
    "\n",
    "OUTPUT_PREFIX = \"tutorial_dense\" # Name for model\n",
    "\n",
    "BASE_DIRECTORY = \"./tutorial_dense/version_0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e05810-ae82-4de9-b1f9-cf157c4ee7b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: ./tutorial_dense/version_0/checkpoints/epoch=0-step=28500.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint and add the test file location\n",
    "options = Options.load(f\"{BASE_DIRECTORY}/options.json\")\n",
    "options.testing_file = options.training_file.replace(\"training\", TESTING_SOURCE)\n",
    "options.num_dataloader_workers = 0\n",
    "options.transformer_norm_first = bool(options.transformer_norm_first)\n",
    "\n",
    "if CHECKPOINT_PATH is None:\n",
    "    checkpoints = glob(f\"{BASE_DIRECTORY}/checkpoints/epoch*.ckpt\")\n",
    "    last_checkpoint = np.argmax([int(re.search(\"step=(.*).ckpt\", s)[1]) for s in checkpoints])\n",
    "    checkpoint_path = checkpoints[last_checkpoint]\n",
    "else:\n",
    "    checkpoint_path = CHECKPOINT_PATH\n",
    "    \n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "print(f\"Loading from: {checkpoint_path}\")\n",
    "\n",
    "network = NETWORK(options)\n",
    "network.load_state_dict(state_dict)\n",
    "\n",
    "network = network.eval()\n",
    "for parameter in network.parameters():\n",
    "    parameter.requires_grad_(False)\n",
    "    \n",
    "if CUDA:\n",
    "    network = network.cuda(CUDA_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "478d825c-78ef-4e5f-a228-fc2183c39d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    features,\n",
    "    extra,\n",
    "    event_coordinates,\n",
    "    event_pixel_values,\n",
    "    event_masks,\n",
    "    prong_coordinates,\n",
    "    prong_pixel_values,\n",
    "    prong_masks,\n",
    "    event_targets,\n",
    "    prong_targets\n",
    ") = next(iter(DataLoader(network.testing_dataset, batch_size=1, collate_fn=network.dataloader_options[\"collate_fn\"])))\n",
    "\n",
    "max_prongs_in_batch = prong_masks.sum(1).max()\n",
    "features = features[:, :max_prongs_in_batch].contiguous()\n",
    "prong_masks = prong_masks[:, :max_prongs_in_batch].contiguous()\n",
    "prong_targets = prong_targets[:, :max_prongs_in_batch].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd9812db-f200-4350-86a7-2768bded34e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "event_pixels = 255 * network.preprocess_pixels(event_coordinates, event_pixel_values, network.training_dataset.pixel_shape).to_dense()\n",
    "prong_pixels = 255 * network.preprocess_pixels(prong_coordinates, prong_pixel_values, network.training_dataset.pixel_shape).to_dense()\n",
    "pixels = torch.cat((event_pixels, prong_pixels), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61c9fbcf-2ff2-449d-91b6-e2d06e209f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "event_preds, prong_preds = network(\n",
    "    features,\n",
    "    extra,\n",
    "    event_coordinates,\n",
    "    event_pixel_values,\n",
    "    event_masks,\n",
    "    prong_coordinates,\n",
    "    prong_pixel_values,\n",
    "    prong_masks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93b529fd-176f-478b-a688-8fef5a541f51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DynamicSimplifedNetwork(nn.Module):\n",
    "    __constants__ = [\"pixel_features\", \"pixel_width\", \"pixel_height\", \"num_features\", \"num_extra\"]\n",
    "    def __init__(self, network):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = network.network\n",
    "        \n",
    "        self.num_features = network.training_dataset.num_features\n",
    "        self.num_extra = network.training_dataset.extra.shape[1]\n",
    "        \n",
    "        self.pixel_features = network.training_dataset.pixel_features\n",
    "        self.pixel_width = network.training_dataset.pixel_shape[0]\n",
    "        self.pixel_height = network.training_dataset.pixel_shape[1]\n",
    "        \n",
    "        self.extra_mean = network.extra_mean\n",
    "        self.extra_std = network.extra_std\n",
    "        \n",
    "        self.mean = network.mean\n",
    "        self.std = network.std\n",
    "        \n",
    "        self.log_pixels = network.options.log_pixels\n",
    "        \n",
    "    def forward(self, pixels):\n",
    "        if self.log_pixels:\n",
    "            pixels = torch.log(pixels.float() + 1)\n",
    "        else:\n",
    "            pixels = pixels.float() / 255\n",
    "            \n",
    "        pixels = pixels.reshape(\n",
    "            -1, \n",
    "            self.pixel_features,\n",
    "            self.pixel_width,\n",
    "            self.pixel_height\n",
    "        )\n",
    "                \n",
    "        num_images = pixels.shape[0]\n",
    "        \n",
    "        # Create Artificial Data\n",
    "        mask = torch.ones(num_images, device=pixels.device, dtype=torch.bool)\n",
    "        features = torch.zeros(1, num_images - 1, self.num_features, device=pixels.device, dtype=pixels.dtype)\n",
    "        extra = torch.zeros(1, self.num_extra, device=pixels.device, dtype=pixels.dtype)\n",
    "        \n",
    "        event_pixels, prong_pixels = pixels[:1], pixels[1:]\n",
    "        event_mask, prong_mask = mask[:1], mask[1:]\n",
    "        \n",
    "        event, prongs = self.network(\n",
    "            features, \n",
    "            extra, \n",
    "            event_pixels, \n",
    "            event_mask.unsqueeze(0), \n",
    "            prong_pixels, \n",
    "            prong_mask.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        event = torch.softmax(event[0], 0)\n",
    "        prongs = torch.softmax(prongs[0], 1)\n",
    "        \n",
    "        if event.shape[-1] > 4:\n",
    "            event = torch.stack((\n",
    "                event[:4].sum(), \n",
    "                event[4:8].sum(), \n",
    "                event[8], \n",
    "                event[9], \n",
    "            ), dim=0)\n",
    "        \n",
    "        return event, prongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56bb22d3-7664-4050-a54f-e16008324a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DynamicEmbeddingNetwork(nn.Module):\n",
    "    __constants__ = [\"pixel_features\", \"pixel_width\", \"pixel_height\", \"num_features\", \"num_extra\"]\n",
    "    def __init__(self, network):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = network.network\n",
    "        \n",
    "        self.num_features = network.training_dataset.num_features\n",
    "        self.num_extra = network.training_dataset.extra.shape[1]\n",
    "        \n",
    "        self.pixel_features = network.training_dataset.pixel_features\n",
    "        self.pixel_width = network.training_dataset.pixel_shape[0]\n",
    "        self.pixel_height = network.training_dataset.pixel_shape[1]\n",
    "        \n",
    "        self.extra_mean = network.extra_mean\n",
    "        self.extra_std = network.extra_std\n",
    "        \n",
    "        self.mean = network.mean\n",
    "        self.std = network.std\n",
    "        \n",
    "        self.log_pixels = network.options.log_pixels\n",
    "    \n",
    "    def forward(self, pixels):\n",
    "        if self.log_pixels:\n",
    "            pixels = torch.log(pixels.float() + 1)\n",
    "        else:\n",
    "            pixels = pixels.float() / 255\n",
    "            \n",
    "        pixels = pixels.reshape(\n",
    "            -1, \n",
    "            self.pixel_features,\n",
    "            self.pixel_width,\n",
    "            self.pixel_height\n",
    "        )\n",
    "                \n",
    "        num_images = pixels.shape[0]\n",
    "        \n",
    "        # Create Artificial Data\n",
    "        mask = torch.ones(num_images, device=pixels.device, dtype=torch.bool)\n",
    "        features = torch.zeros(1, num_images - 1, self.num_features, device=pixels.device, dtype=pixels.dtype)\n",
    "        extra = torch.zeros(1, self.num_extra, device=pixels.device, dtype=pixels.dtype)\n",
    "        \n",
    "        event_pixels, prong_pixels = pixels[:1], pixels[1:]\n",
    "        event_mask, prong_mask = mask[:1], mask[1:]\n",
    "        \n",
    "        \n",
    "        combined_embeddings, combined_mask = self.network.prong_embedding(\n",
    "            features, \n",
    "            extra, \n",
    "            event_pixels, \n",
    "            event_mask.unsqueeze(0), \n",
    "            prong_pixels, \n",
    "            prong_mask.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        combined_embeddings, _, _ = self.network.encoder(combined_embeddings, combined_mask)\n",
    "\n",
    "        event_features, prong_features = combined_embeddings[0, 0], combined_embeddings[1:, 0]        \n",
    "        return event_features, prong_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4f694c5-044e-4438-8939-bcb0a8d632e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DynamicCombinedNetwork(nn.Module):\n",
    "    __constants__ = [\"pixel_features\", \"pixel_width\", \"pixel_height\", \"num_features\", \"num_extra\"]\n",
    "    def __init__(self, network):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = network.network\n",
    "        \n",
    "        self.num_features = network.training_dataset.num_features\n",
    "        self.num_extra = network.training_dataset.extra.shape[1]\n",
    "        \n",
    "        self.pixel_features = network.training_dataset.pixel_features\n",
    "        self.pixel_width = network.training_dataset.pixel_shape[0]\n",
    "        self.pixel_height = network.training_dataset.pixel_shape[1]\n",
    "        \n",
    "        self.extra_mean = network.extra_mean\n",
    "        self.extra_std = network.extra_std\n",
    "        \n",
    "        self.mean = network.mean\n",
    "        self.std = network.std\n",
    "        \n",
    "        self.log_pixels = network.options.log_pixels\n",
    "    \n",
    "    def forward(self, pixels):\n",
    "        if self.log_pixels:\n",
    "            pixels = torch.log(pixels.float() + 1)\n",
    "        else:\n",
    "            pixels = pixels.float() / 255\n",
    "            \n",
    "        pixels = pixels.reshape(\n",
    "            -1, \n",
    "            self.pixel_features,\n",
    "            self.pixel_width,\n",
    "            self.pixel_height\n",
    "        )\n",
    "                \n",
    "        num_images = pixels.shape[0]\n",
    "        \n",
    "        # Create Artificial Data\n",
    "        mask = torch.ones(num_images, device=pixels.device, dtype=torch.bool)\n",
    "        features = torch.zeros(1, num_images - 1, self.num_features, device=pixels.device, dtype=pixels.dtype)\n",
    "        extra = torch.zeros(1, self.num_extra, device=pixels.device, dtype=pixels.dtype)\n",
    "        \n",
    "        event_pixels, prong_pixels = pixels[:1], pixels[1:]\n",
    "        event_mask, prong_mask = mask[:1], mask[1:]\n",
    "        \n",
    "        \n",
    "        combined_embeddings, combined_mask = self.network.prong_embedding(\n",
    "            features, \n",
    "            extra, \n",
    "            event_pixels, \n",
    "            event_mask.unsqueeze(0), \n",
    "            prong_pixels, \n",
    "            prong_mask.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        hidden_features, padding_mask, sequence_mask = self.network.encoder(combined_embeddings, combined_mask)\n",
    "        \n",
    "        event_features, prong_features = hidden_features[0], hidden_features[1:]\n",
    "        \n",
    "        event = self.network.event_decoder(event_features)\n",
    "        prongs = self.network.prong_decoder(prong_features).transpose(0, 1)\n",
    "        \n",
    "        event_features, prong_features = event_features[0], prong_features[:, 0]\n",
    "        event = torch.softmax(event[0], 0)\n",
    "        prongs = torch.softmax(prongs[0], 1)\n",
    "        \n",
    "        if event.shape[-1] > 4:\n",
    "            event = torch.stack((\n",
    "                event[:4].sum(), \n",
    "                event[4:8].sum(), \n",
    "                event[8], \n",
    "                event[9], \n",
    "            ), dim=0)\n",
    "            \n",
    "        return event, prongs, event_features, prong_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c278ad5b-c7fd-4983-a158-a760a91bc8cb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/exp/dune/data/users/ayankele/transformercvn_env/lib/python3.10/site-packages/torch/jit/_recursive.py:266: UserWarning: 'batch_first' was found in ScriptModule constants, but was not actually set in __init__. Consider removing it.\n",
      "  warnings.warn(\"'{}' was found in ScriptModule constants, \"\n"
     ]
    }
   ],
   "source": [
    "dynamic_simplified = torch.jit.script(DynamicSimplifedNetwork(network))\n",
    "dynamic_embeddings = torch.jit.script(DynamicEmbeddingNetwork(network))\n",
    "dynamic_combined = torch.jit.script(DynamicCombinedNetwork(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94764d73-a4ce-417b-ac81-7ba3ec00513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure the traced models work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "398c6d52-cde9-43b1-af64-41a0dfc09497",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 3, 400, 280]) -> torch.Size([4]) , torch.Size([6, 8])\n"
     ]
    }
   ],
   "source": [
    "# This model will output two tensors:\n",
    "#  1. The event classification probabilities\n",
    "#  2. The prong classification probabilities for each input prong image\n",
    "\n",
    "outs = dynamic_simplified(pixels)\n",
    "print(pixels.shape, '->', outs[0].shape, ',', outs[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c222d3a4-06f8-4074-a2bb-b16de2cf977d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 3, 400, 280]) -> torch.Size([128]) , torch.Size([6, 128])\n"
     ]
    }
   ],
   "source": [
    "# This model will output the intermediate feature representation hidden vectors \n",
    "# that serve as input to the final classification layers. Each are length 128.\n",
    "# This model outputs two tensors:\n",
    "#  1. The vector representing the event image\n",
    "#  2. The vectors representing each input prong image\n",
    "outs = dynamic_embeddings(pixels)\n",
    "print(pixels.shape, '->', outs[0].shape, ',', outs[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff14628d-d5d0-4eec-b9e5-79ff46f1c64c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 3, 400, 280]) -> torch.Size([4]) , torch.Size([6, 8]) , torch.Size([128]) , torch.Size([6, 128])\n"
     ]
    }
   ],
   "source": [
    "# This model outputs four tensors (all of the above):\n",
    "#  1. The event classification probabilities\n",
    "#  2. The prong classification probabilities for each input prong image\n",
    "#  3. The vector representing the event image\n",
    "#  4. The vectors representing each input prong image\n",
    "outs = dynamic_combined(pixels)\n",
    "print(pixels.shape, '->', ' , '.join([str(out.shape) for out in outs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3f5a134-3e6d-4402-8c9e-67c0895c2403",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dynamic_simplified.save(f\"{BASE_DIRECTORY}/{OUTPUT_PREFIX}_{checkpoint_path.split('/')[-1].split('.')[0]}_pid.torchscript\")\n",
    "dynamic_embeddings.save(f\"{BASE_DIRECTORY}/{OUTPUT_PREFIX}_{checkpoint_path.split('/')[-1].split('.')[0]}_embeddings.torchscript\")\n",
    "dynamic_combined.save(f\"{BASE_DIRECTORY}/{OUTPUT_PREFIX}_{checkpoint_path.split('/')[-1].split('.')[0]}_combined.torchscript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2a95c-b388-486c-8cd1-1770691e07d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformercvn_env",
   "language": "python",
   "name": "transformercvn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
